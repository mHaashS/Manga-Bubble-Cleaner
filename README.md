# Manga-Bubble-Cleaner
### Objectif global
CrÃ©er un pipeline automatisÃ© pour :

- DÃ©tecter les bulles dans des pages de manga (bubbles, narration, floating text)
- Nettoyer ces bulles (effacement ou inpainting)
- Extraire le texte original (OCR)
- Le traduire automatiquement
- GÃ©nÃ©rer des fichiers .txt + .json qui contiennent le texte original et traduit (au cas oÃ¹ la traduction automatique est incorrecte ou incomplÃ¨te) et la position des bulles
- RÃ©insÃ©rer le texte dans lâ€™image avec la position dans le json

## Ã‰tape 1 â€” EntraÃ®nement du modÃ¨le
Objectif :
DÃ©tecter automatiquement les bulles dans les pages de manga avec un modÃ¨le Mask R-CNN personnalisÃ©.

ğŸ§ª Mise en Å“uvre
Annotations faites via CVAT, exportÃ©es au format COCO JSON

Les classes sont :
- bubble (attributs : normal, cri, malade)
- floating_text (attributs : titre, cri, narration)
- narration_box (sans attribut)

Lors de l'annotation des donnÃ©es sur CVAT, j'avais initialement ajoutÃ© des attributs personnalisÃ©s Ã  certaines classes, comme :
bubble â†’ {normal, cri, malade}
floating_text â†’ {titre, narration, cri}
Cependant, je me suis rendu compte trop tard que Detectron2 ne prend pas en compte les attributs COCO dans son format d'entraÃ®nement.
Seuls les labels (classes) sont utilisÃ©s, ce qui signifie que ces attributs ont Ã©tÃ© ignorÃ©s lors de l'entraÃ®nement.

Le script enregistre deux datasets :
register_coco_instances("manga_train", {}, annotations_train.json, images/)
register_coco_instances("manga_val", {}, annotations_val.json, images/)
Le modÃ¨le Mask R-CNN est entraÃ®nÃ© avec les 3 classes.

Le modÃ¨le est sauvegardÃ© dans : output/model_final.pth

## Ã‰tape 2 â€” Observation des prÃ©dictions du modÃ¨le
AprÃ¨s avoir entraÃ®nÃ© mon modÃ¨le, jâ€™ai voulu mâ€™assurer quâ€™il Ã©tait capable de reconnaÃ®tre visuellement les bulles dans une page de manga.
Pour cela, jâ€™ai prÃ©parÃ© quelques images de test, puis jâ€™ai lancÃ© le modÃ¨le pour voir oÃ¹ et comment il dÃ©tectait les objets.

Lâ€™objectif nâ€™Ã©tait pas encore de nettoyer ou traduire quoi que ce soit, mais simplement de :
- Valider que les bulles Ã©taient bien reconnues
- Voir si les masques collaient bien aux contours rÃ©els
- Et vÃ©rifier si le modÃ¨le faisait des erreurs (oubli, confusion...)

Jâ€™ai visualisÃ© les prÃ©dictions sur les images avec des couleurs diffÃ©rentes pour chaque type de bulle (bulles classiques, floating text, narration).
Cela mâ€™a permis de juger rapidement :
- Si la qualitÃ© de l'entraÃ®nement Ã©tait suffisante
- Si les annotations initiales Ã©taient cohÃ©rentes
- Et si je pouvais passer Ã  lâ€™Ã©tape suivante

Pour les pages avec des bulles simples, le modÃ¨le arrive facilement Ã  reconnaitre les bulles.

![image1](https://github.com/user-attachments/assets/121673fe-a03b-4f78-9d34-e18871854b21)

La tÃ¢che se complique un peu quand il s'attaque Ã  des floating_text.
Surement du au fait qu'il y avait moins de data avec des floating_text.

![image2](https://github.com/user-attachments/assets/30997745-2115-4465-b0b0-148027ca5779)


## Ã‰tape 3 â€” Nettoyage visuel des bulles
Une fois que jâ€™Ã©tais satisfait de la dÃ©tection des bulles, jâ€™ai voulu effacer le texte quâ€™elles contiennent.
Lâ€™objectif ici nâ€™Ã©tait pas encore dâ€™extraire le texte, mais simplement de vider les bulles pour pouvoir y insÃ©rer du texte plus tard.

PlutÃ´t que de supprimer toutes les zones dÃ©tectÃ©es de la mÃªme maniÃ¨re, jâ€™ai choisi une approche adaptÃ©e au type de bulle :
- Pour les bulles classiques et les boÃ®tes de narration, jâ€™ai simplement rempli la zone avec une couleur blanche.
- Pour les floating text (souvent du texte sans contour net), jâ€™ai utilisÃ© une technique qui essaie de reconstruire le fond de lâ€™image en supprimant le texte (inpainting) mais qui n'est pas au point.

Cette Ã©tape mâ€™a permis de visualiser une page nettoyÃ©e de tout son texte, tout en gardant la structure des bulles intacte
Cela a Ã©tÃ© essentiel pour prÃ©parer lâ€™insertion future du texte traduit.
Jâ€™ai aussi fixÃ© un seuil de confiance Ã  75% pour ne nettoyer que les bulles dont le modÃ¨le Ã©tait suffisamment sÃ»r, afin dâ€™Ã©viter dâ€™effacer des parties incorrectes.

![image3](https://github.com/user-attachments/assets/693c22b3-4398-4798-8222-fa7ae7d91cb5)
![image4](https://github.com/user-attachments/assets/c04343a7-7479-4693-8a9e-76a2465fc467)

## Ã‰tape 4 â€” Extraction du texte + Traduction automatique
Une fois les bulles dÃ©tectÃ©es sur une page de manga, jâ€™ai automatisÃ© un processus en deux temps :
ğŸ” lire le texte prÃ©sent dans chaque bulle, puis ğŸŒ le traduire automatiquement en franÃ§ais.

Jâ€™ai regroupÃ© ces deux Ã©tapes dans un seul script, qui prend une image en entrÃ©e et produit un fichier .txt et .json comme sortie.

ğŸ§ª Ã‰tapes du traitement
Pour chaque bulle dÃ©tectÃ©e :
- Extraction de la zone Ã  partir du masque de segmentation
- DÃ©coupage de la zone dans lâ€™image (ROI)

Application dâ€™EasyOCR :
- OCR robuste, sans prÃ©traitement nÃ©cessaire
- Capable de lire du texte stylisÃ© et irrÃ©gulier
- RÃ©sultat brut nettoyÃ© (espaces multiples, retours Ã  la ligne, etc.)

Traduction automatique :
- Traduction du texte anglais vers le franÃ§ais
- RÃ©alisÃ©e via lâ€™API OpenAI (GPT-3.5)
- Le texte original et sa traduction sont tous deux enregistrÃ©s

Export des rÃ©sultats :
- En .txt lisible pour lâ€™utilisateur
- En .json structurÃ© pour usage automatisÃ©
  
![image5](https://github.com/user-attachments/assets/89ffcd4e-02e1-4dfa-bb3c-f1537178c068)

## Ã‰tape 5 â€” RÃ©insertion du texte traduit dans les bulles

AprÃ¨s avoir dÃ©tectÃ©, nettoyÃ© et traduit les bulles de texte dans les pages de manga, lâ€™objectif final est de **rÃ©insÃ©rer automatiquement le texte traduit dans lâ€™image nettoyÃ©e**, Ã  lâ€™endroit exact oÃ¹ se trouvait le texte original.

Cette Ã©tape transforme rÃ©ellement le pipeline : on ne se contente plus dâ€™un fichier `.txt`, mais on recrÃ©e une **image complÃ¨te, lisible et localisÃ©e**.

---

ğŸ¯ **Objectif**

- Reconstituer visuellement une version "localisÃ©e" des pages manga
- PrÃ©server les bulles, le fond, et l'esthÃ©tique gÃ©nÃ©rale
- RÃ©utiliser les coordonnÃ©es des bulles extraites lors de la dÃ©tection

---

ğŸ§ª **DÃ©marche mise en place (script `reinsert_translations.py`)**

1.  **Chargement des donnÃ©es**
   - Image nettoyÃ©e (gÃ©nÃ©rÃ©e Ã  lâ€™Ã©tape 3)
   - Fichier `.json` contenant les traductions + coordonnÃ©es de chaque bulle

2.  **Utilisation des coordonnÃ©es de la bulle**
   - Chaque bulle possÃ¨de des coordonnÃ©es (`x_min`, `y_min`, `x_max`, `y_max`)
   - Ces donnÃ©es sont utilisÃ©es pour **positionner** le texte correctement dans la zone correspondante

3.  **RÃ©insertion du texte avec centrage automatique**
   - Le script mesure la largeur/hauteur disponibles
   - Il ajuste dynamiquement la **taille de la police** pour que le texte tienne dans la bulle
   - Le texte est centrÃ© automatiquement dans lâ€™espace prÃ©vu

4.  **Typographie adaptable**
   - Par dÃ©faut, une police classique (`arial.ttf`) est utilisÃ©e
   - Si indisponible, le script bascule sur la police systÃ¨me par dÃ©faut
   - Ã€ terme, on pourrait ajuster la police selon le type de bulle

5.  **Export automatique**
   - Lâ€™image finale est sauvegardÃ©e sous un nouveau nom (`image_clean_translated.png`)
   - Lâ€™ensemble du processus est automatisÃ©

---

ğŸ“¸ *Exemple visuel : avant / aprÃ¨s rÃ©insertion*  

---

### Cloner le dÃ©pÃ´t

```bash
# Clonez le repo
git clone https://github.com/mHaashS/Manga-Bubble-Cleaner.git

# Placez-vous dans le dossier
cd Manga-Bubble-Cleaner

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
mkdir -p models
curl -L https://github.com/mHaashS/Manga-Bubble-Cleaner/releases/latest/download/model_final.pth \
     -o models/model_final.pth

# Clean Bubbles
python scripts/clean_bubbles.py path/to/image.png

# Translate Bubbles
python scripts/translate_bubble.py path/to/image.png

# Reinsert Translation
python scripts/reinsert_translation.py path/to/image_cleaned.png path/to/.translation.json
```

ğŸ“¦ Technologies utilisÃ©es
- Outil / Librairie	RÃ´le
- Python 3.10:	 Langage principal du projet
- Detectron2: 	DÃ©tection des bulles avec Mask R-CNN (https://github.com/matterport/Mask_RCNN)
- EasyOCR:	Extraction de texte dans les bulles
- OpenCV: 	Traitement dâ€™images, masquage et nettoyage
- Pillow (PIL):	RÃ©insertion du texte dans lâ€™image
- OpenAI API:	Traduction automatique via GPT-3.5
- CVAT:	Annotation des donnÃ©es au format COCO
- json / txt export:	Format de sauvegarde des rÃ©sultats
